{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'twitter_api'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-29bb46752bfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtwitter_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_twitter_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'twitter_api'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set()\n",
    "import os\n",
    "from twitter_api import get_twitter_data\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "# Do not truncate data in cells\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Do not limit number of displayed columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display floats with 2 decimals\n",
    "pd.set_option('display.float_format', '{:20,.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'twitter-archive-enhanced.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c7720056d6fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load twitter archive file into pandas df.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_arch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'twitter-archive-enhanced.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'twitter-archive-enhanced.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#load twitter archive file into pandas df.\n",
    "df_arch = pd.read_csv('twitter-archive-enhanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download image predictions file.\n",
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = requests.get(url)\n",
    "with open(os.path.join('data/' + url.split('/')[-1]), 'wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image predictions file into pandas df.\n",
    "df_pred = pd.read_csv('data/image-predictions.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download data from twitter API \n",
    "if not os.path.exists('data/tweet_json.txt'):\n",
    "    get_twitter_data(df_arch, 'data/tweet_json.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tweets data into pandas df\n",
    "with open('data/tweet_json.txt') as file:\n",
    "    df_api = pd.read_json(file, lines= True, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arch\n",
    "\n",
    "df_arch.info()\n",
    "\n",
    "\n",
    "df_arch[['rating_numerator', 'rating_denominator']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arch.name.unique()\n",
    "\n",
    "df_arch.name.value_counts()\n",
    "\n",
    "df_arch[df_arch.name.str.islower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for numerators with wrong values in chunks\n",
    "df_arch[df_arch.rating_numerator <= 5].loc[0:1000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arch[df_arch.rating_numerator <= 5].loc[1000:2000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arch[df_arch.rating_numerator <= 5].loc[2000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.info()\n",
    "\n",
    "df_pred.columns\n",
    "\n",
    "df_pred.img_num.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.p1.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api\n",
    "\n",
    "df_api.info()\n",
    "\n",
    "df_api.lang.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Quality\n",
    "1.Columns (doggo, floofer, pupper, puppo) has None for missing values.\n",
    "2.source column is html tag <a> we can extract the source of the tweet and covert it to categorical.\n",
    "3.text column has the link for the tweets and ratings at the end we can remove it.\n",
    "4.timestamp column is str instead of datetime\n",
    "5.We are interested in the tweet ONLY not the retweet there for we should remove those from the table.\n",
    "6.We are interested in the tweet ONLY not the reply to the original tweet there for we should remove those from the table.\n",
    "7.The rating_numerator column should of type float and also it should be correctly extracted.\n",
    "8.rating_denominator column has values less than 10 and values more than 10 for ratings more than one dog.\n",
    "9.expanded_urls column has NaN values\n",
    "10.id column in df_api name different than the other 2 data sets.\n",
    "11.name column have None instead of NaN and too many unvalid values.\n",
    "\n",
    "Tidiness\n",
    "\n",
    "doggo, floofer, pupper, puppo columns are all about the same things, a kind of dog personality.\n",
    "img_num useless.\n",
    "the columns (p1, p1_dog, p1_conf, ...etc) should be just breed and confidence.\n",
    "Just 3 columns needed id, retweet_count, favorite_count\n",
    "All datasets should be combined into 1 dataset only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "\n",
    "for col in col_list:\n",
    "    df_arch_cleaned[col] = df_arch_cleaned[col].replace('None', np.nan)\n",
    "\n",
    "df_arch_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the unique values\n",
    "df_arch_cleaned.source.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make a function fix_source which extract the strings between tags\n",
    "def fix_source(x):\n",
    "    'x is an html string from the source column in df_arch_cleaned dataset'\n",
    "    #find the first closed  tag >\n",
    "    i= x.find('>') + 1\n",
    "    # find the first open tag after the previous <\n",
    "    j =x[i:].find('<')\n",
    "    # extract the text in between\n",
    "    return x[i:][:j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arch_cleaned.source = df_arch_cleaned.source.apply(lambda x: fix_source(x)).astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the unique values\n",
    "df_arch_cleaned.source.unique("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arch_cleaned[df_arch_cleaned.text.str.contains(r\"(\\d+\\.\\d*\\/\\d+)\")][['text', 'rating_numerator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ratings = df_arch_cleaned[df_arch_cleaned.text.str.contains(r\"(\\d+\\.\\d*\\/\\d+)\")]['text'].str.extract(r\"(\\d+\\.\\d*(?=\\/\\d+))\")\n",
    "new_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arch_cleaned.loc[new_ratings.index, 'rating_numerator'] = new_ratings.values\n",
    "df_arch_cleaned.rating_numerator = df_arch_cleaned.rating_numerator.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arch_cleaned.loc[new_ratings.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arch_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove ratings and links from text column using RegEx\n",
    "df_arch_cleaned.text[0]\n",
    "df_arch_cleaned.text = df_arch_cleaned.text.str.extract('(.+(?=\\s\\d+/\\d+\\s))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arch_cleaned.text.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converte timestamp column to datetime.\n",
    "\n",
    "df_arch_cleaned.timestamp = pd.to_datetime(df_arch_cleaned.timestamp)\n",
    "df_arch_cleaned.timestamp.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove retweets and replies\n",
    "\n",
    "df_arch_cleaned = df_arch_cleaned.query('in_reply_to_status_id == \"NaN\" &\\\n",
    "                                                     in_reply_to_user_id == \"NaN\" &\\\n",
    "                                                    retweeted_status_id == \"NaN\" &\\\n",
    "                                                    retweeted_status_user_id == \"NaN\"')\n",
    "# drop columns\n",
    "cols = ['in_reply_to_status_id','in_reply_to_user_id','retweeted_status_id',\n",
    "           'retweeted_status_user_id', 'retweeted_status_timestamp']\n",
    "df_arch_cleaned.drop(columns = cols, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for Null values in the df_arch_cleaned\n",
    "df_arch_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove values other than 10 for rating_denominator\n",
    "\n",
    "df_arch_cleaned = df_arch_cleaned[df_arch_cleaned['rating_denominator'] == 10]\n",
    "df_arch_cleaned[['rating_numerator', 'rating_denominator']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove any rows not related to dogs\n",
    "df_arch_cleaned = df_arch_cleaned[~df_arch_cleaned.text.isnull()]\n",
    "df_arch_cleaned = df_arch_cleaned.loc[~df_arch_cleaned.text.str.match('.*only rate dogs')]\n",
    "\n",
    "df_arch_cleaned.loc[df_arch_cleaned.text.str.match('.*only rate dogs')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arch_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with NaNs for expanded_urls column.\n",
    "\n",
    "df_arch_cleaned = df_arch_cleaned.loc[~df_arch_cleaned.expanded_urls.isnull()]\n",
    "df_arch_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dog_stage column and remove the (doggo, floofer, pupper, puppo) columns.\n",
    "# select the dog stages columns from the dataset\n",
    "cols = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "\n",
    "# create the dog_stage column with joining the four columns in one column dog_stage join for more than stage\n",
    "df_arch_cleaned['dog_stage'] = df_arch_cleaned[cols].\\\n",
    "                                        apply(lambda x: ', '.join(x.dropna().astype(str)),axis =1)\n",
    "# replace the empty string with nan and change datatype to category\n",
    "df_arch_cleaned.dog_stage = df_arch_cleaned.dog_stage.replace('', np.nan).astype('category')\n",
    "\n",
    "# drop the 4 columns\n",
    "df_arch_cleaned = df_arch_cleaned.drop(columns = cols, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arch_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arch_cleaned.dog_stage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace 'None' with np.name in df_arch name column.\n",
    "#Remove any rows with invalid names which starts with lower laters.\n",
    "df_arch_cleaned[~df_arch_cleaned.name.str.istitle()].name.unique()\n",
    "df_arch_cleaned.name.replace(['such', 'a', 'quite', 'not', 'one', 'incredibly', 'mad',\n",
    "       'an', 'very', 'just', 'my', 'his', 'actually', 'getting',\n",
    "       'this', 'unacceptable', 'all', 'old', 'infuriating', 'the',\n",
    "       'by', 'officially', 'life', 'light', 'space', 'None'], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arch_cleaned.name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arch_cleaned.name.value_counts()\n",
    "df_arch_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove img_num column from df_pred_cleand\n",
    "df_pred_cleaned.drop('img_num', axis=1, inplace=True)\n",
    "df_pred_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create breed and confidence columns with highest confidence predictions and drop other columns\n",
    "\n",
    "breed = []\n",
    "confidence = []\n",
    "# iterating over df_pred row by row and taking the highest confident prediction other wise np.nan\n",
    "for index, row in df_pred_cleaned.iterrows():\n",
    "    if row['p1_dog'] and row['p1_conf'] == max([row['p1_conf'], row['p2_conf'], row['p3_conf']]):\n",
    "        breed.append(row['p1'])\n",
    "        confidence.append(row['p1_conf'])\n",
    "    elif row['p2_dog'] and row['p2_conf'] == max([row['p1_conf'], row['p2_conf'], row['p3_conf']]):\n",
    "        breed.append(row['p2'])\n",
    "        confidence.append(row['p2_conf'])\n",
    "    elif row['p3_dog'] and row['p3_conf'] == max([row['p1_conf'], row['p2_conf'], row['p3_conf']]):\n",
    "        breed.append(row['p3'])\n",
    "        confidence.append(row['p3_conf'])\n",
    "    else:\n",
    "        breed.append(np.nan)\n",
    "        confidence.append(np.nan)\n",
    "        \n",
    "df_pred_cleaned['breed'] = breed\n",
    "df_pred_cleaned['confidence'] = confidence\n",
    "\n",
    "df_pred_cleaned = df_pred_cleaned[['tweet_id', 'jpg_url', 'breed', 'confidence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_cleaned.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove unnecessary columns for df_api_cleand\n",
    "df_api_cleaned.columns\n",
    "\n",
    "df_api_cleaned = df_api_cleaned[['id', 'retweet_count', 'favorite_count']]\n",
    "\n",
    "df_api_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename id column in df_api_cleand to tweet_id\n",
    "df_api_cleaned.columns = ['tweet_id', 'retweet_count', 'favorite_count']\n",
    "\n",
    "df_api_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge data into database\n",
    "df = pd.merge(df_arch_cleaned, df_pred_cleaned, on='tweet_id')\n",
    "df = pd.merge(df, df_api_cleaned, on = 'tweet_id')\n",
    "\n",
    "# Create SQLAlchemy Engine and empty twitter_archive_master database\n",
    "engine = create_engine('sqlite:///data/twitter_archive_master.db')\n",
    "# Store master df into table master \n",
    "try:\n",
    "    df.to_sql('master', engine, index=False)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = pd.read_sql('SELECT * FROM master', engine, parse_dates='timestamp')\n",
    "df_master.source = df_master.source.astype('category')\n",
    "df_master.dog_stage = df_master.dog_stage.astype('category')\n",
    "df_master.breed = df_master.breed.astype('category')\n",
    "df_master.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ratings distribution by ploting a bar chart for it's frequency.\n",
    "\n",
    "data = df_master.rating_numerator.value_counts()\n",
    "\n",
    "x = data.index\n",
    "y = data.values\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "g = sns.barplot(x, y, palette='Blues_d', ax=ax)\n",
    "ax.set(xlabel='Ratings', ylabel='Frequency', title='Ratings frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution with box plot.\n",
    "\n",
    "data = df_master.rating_numerator.value_counts()\n",
    "\n",
    "ax = sns.boxplot(data, orient='v', width=.4)\n",
    "ax.set(xlabel='Ratings', ylabel='Frequency', title='Ratings frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see that their are 2 outliers here so let's investigate more and check their data\n",
    "\n",
    "outliers_df = df_master[df_master.rating_numerator > 400][['rating_numerator', 'name', 'jpg_url', 'text']]\n",
    "outliers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')\n",
    "fig=plt.figure()\n",
    "c = 1\n",
    "for index, row in outliers_df.iterrows():\n",
    "    r = requests.get(row['jpg_url'])\n",
    "    i = Image.open(BytesIO(r.content))\n",
    "    i.save('images/' +  str(index) + '_' + str(row['rating_numerator']) + \"_\" + str(row['name']) + '.jpg')\n",
    "    fig.add_subplot(1, 2, c)\n",
    "    c += 1\n",
    "    plt.imshow(i)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot to show the relation between favorits and retweets\n",
    "ax = sns.scatterplot(x='retweet_count', y='favorite_count', data=df_master,\n",
    "                     hue='rating_numerator', hue_norm=(5, 20), s=15) \n",
    "ax.set(xlabel='Retweet count', ylabel='Favorite count', title='Favorits VS Retweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x='retweet_count', y='favorite_count', data=df_master, color='b', scatter_kws={'s':5, 'alpha':.3}) \n",
    "ax.set(xlabel='Retweet count', ylabel='Favorite count', title='Favorits VS Retweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare dog stages\n",
    "\n",
    "data = df_master.groupby('dog_stage').count()['tweet_id']\n",
    "ax = sns.barplot(y=data.index, x=data.values, palette='Blues_d')\n",
    "ax.set(xlabel='Count', ylabel='Dog stage', title='Dog Stage Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing tweets different sources\n",
    "\n",
    "data = df_master.groupby('source').count()['tweet_id']\n",
    "ax = sns.barplot(y=data.index, x=data.values, palette='Blues_d')\n",
    "ax.set(xlabel='Count', ylabel='Tweet source', title='Tweet Source Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_sql(\"\"\"SELECT name, COUNT(*) AS count\n",
    "                        FROM master \n",
    "                    GROUP BY 1\n",
    "                      HAVING name <> 'A'\n",
    "                    ORDER BY 2 DESC\n",
    "                       LIMIT 10;\n",
    "                  \"\"\", engine)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "g = sns.barplot(x='name', y='count', data=data, palette='Blues_d', ax=ax)\n",
    "ax.set(ylabel='Count', xlabel='Dog name', title='Most popular dog names')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
