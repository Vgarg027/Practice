{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For downloading files from the web\n",
    "import requests\n",
    "\n",
    "# To work with JSON data\n",
    "import json\n",
    "\n",
    "# For file system operations (optional, but useful)\n",
    "import os\n",
    "\n",
    "# For data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For working with dates and times if needed\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# Project: Wrangling and Analyze Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "In the cell below, gather **all** three pieces of data for this project and load them in the notebook. **Note:** the methods required to gather each data are different.\n",
    "1. Directly download the WeRateDogs Twitter archive data \n",
    "(twitter_archive_enhanced.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_archive = pd.read_csv('twitter-archive-enhanced (5).csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the Requests library to download the tweet image prediction (image_predictions.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Step 1: Download the TSV file from Udacity's URL\n",
    "url = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/November/5a0cbb4d_image-predictions/image-predictions.tsv\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Save the file locally\n",
    "with open('image-predictions.tsv', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Step 3: Load the TSV file into a DataFrame\n",
    "df_image = pd.read_csv('image-predictions.tsv', sep='\\t')\n",
    "\n",
    "# Step 4: Check the structure\n",
    "print(df_image.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the Tweepy library to query additional data via the Twitter API (tweet_json.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip tweet-json.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Create an empty list to hold each tweet's data\n",
    "tweet_list = []\n",
    "\n",
    "# Open the tweet-json.txt file and read line by line\n",
    "with open('tweet-json.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        tweet = json.loads(line)  # Parse each JSON line into a dict\n",
    "        tweet_list.append({\n",
    "            'tweet_id': str(tweet['id']),\n",
    "            'retweet_count': tweet['retweet_count'],\n",
    "            'favorite_count': tweet['favorite_count']\n",
    "        })\n",
    "\n",
    "# Convert the list of dicts into a DataFrame\n",
    "df_tweet = pd.DataFrame(tweet_list)\n",
    "\n",
    "# Optional: Preview the data\n",
    "print(df_tweet.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 28,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Assessing Data\n",
    "In this section, detect and document at least **eight (8) quality issues and two (2) tidiness issue**. You must use **both** visual assessment\n",
    "programmatic assessement to assess the data.\n",
    "\n",
    "**Note:** pay attention to the following key points when you access the data.\n",
    "\n",
    "* You only want original ratings (no retweets) that have images. Though there are 5000+ tweets in the dataset, not all are dog ratings and some are retweets.\n",
    "* Assessing and cleaning the entire dataset completely would require a lot of time, and is not necessary to practice and demonstrate your skills in data wrangling. Therefore, the requirements of this project are only to assess and clean at least 8 quality issues and at least 2 tidiness issues in this dataset.\n",
    "* The fact that the rating numerators are greater than the denominators does not need to be cleaned. This [unique rating system](http://knowyourmeme.com/memes/theyre-good-dogs-brent) is a big part of the popularity of WeRateDogs.\n",
    "* You do not need to gather the tweets beyond August 1st, 2017. You can, but note that you won't be able to gather the image predictions for these tweets since you don't have access to the algorithm used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 28,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Assessing Data\n",
    "In this section, detect and document at least **eight (8) quality issues and two (2) tidiness issue**. You must use **both** visual assessment\n",
    "programmatic assessement to assess the data.\n",
    "\n",
    "**Note:** pay attention to the following key points when you access the data.\n",
    "\n",
    "* You only want original ratings (no retweets) that have images. Though there are 5000+ tweets in the dataset, not all are dog ratings and some are retweets.\n",
    "* Assessing and cleaning the entire dataset completely would require a lot of time, and is not necessary to practice and demonstrate your skills in data wrangling. Therefore, the requirements of this project are only to assess and clean at least 8 quality issues and at least 2 tidiness issues in this dataset.\n",
    "* The fact that the rating numerators are greater than the denominators does not need to be cleaned. This [unique rating system](http://knowyourmeme.com/memes/theyre-good-dogs-brent) is a big part of the popularity of WeRateDogs.\n",
    "* You do not need to gather the tweets beyond August 1st, 2017. You can, but note that you won't be able to gather the image predictions for these tweets since you don't have access to the algorithm used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the three datasets\n",
    "df_archive = pd.read_csv('twitter-archive-enhanced (5).csv')\n",
    "df_image = pd.read_csv('image-predictions.tsv', sep='\\t')\n",
    "\n",
    "tweet_json = []\n",
    "with open('tweet-json.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        tweet_json.append(json.loads(line))\n",
    "df_tweet_json = pd.DataFrame(tweet_json)\n",
    "\n",
    "# Show first 5 rows of each dataset (visual assessment)\n",
    "print(\"Archive data sample:\")\n",
    "print(df_archive.head())\n",
    "\n",
    "print(\"\\nImage predictions sample:\")\n",
    "print(df_image.head())\n",
    "\n",
    "print(\"\\nTweet JSON data sample:\")\n",
    "print(df_tweet_json.head())\n",
    "\n",
    "# Programmatic assessment - quick checks\n",
    "\n",
    "print(\"\\nMissing values in Archive data:\")\n",
    "print(df_archive.isnull().sum())\n",
    "\n",
    "print(\"\\nNumber of retweets (should remove these):\")\n",
    "print(df_archive['retweeted_status_id'].notnull().sum())\n",
    "\n",
    "print(\"\\nTweets without images:\")\n",
    "print(df_archive['expanded_urls'].isnull().sum())\n",
    "\n",
    "print(\"\\nUnique dog names (some may be wrong):\")\n",
    "print(df_archive['name'].value_counts().tail(10))\n",
    "\n",
    "print(\"\\nRating denominator values:\")\n",
    "print(df_archive['rating_denominator'].value_counts())\n",
    "\n",
    "print(\"\\nDog stage columns example:\")\n",
    "print(df_archive[['doggo', 'floofer', 'pupper', 'puppo']].sample(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality issues\n",
    "1. Retweets exist in `twitter_archive`, but we only want original tweets.\n",
    "2. Some tweets have no images (missing `expanded_urls`).\n",
    "3. Invalid dog names like “a”, “an”, “the”, “None”, or lowercase words in `name`.\n",
    "4. `timestamp` column is in string format, not datetime.\n",
    "5. `source` column contains HTML, not readable platform names.\n",
    "6. Ratings have `rating_denominator` not equal to 10.\n",
    "7. Inconsistent capitalization in image prediction names (e.g., “golden_retriever” vs “Golden_retriever”).\n",
    "8. Low confidence values in image predictions (e.g., `p1_conf` < 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 7,
        "hidden": false,
        "row": 40,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "### Tidiness issues\n",
    "1. `doggo`, `floofer`, `pupper`, and `puppo` should be one column: `dog_stage`.\n",
    "2. The three datasets should be merged into one master dataset via `tweet_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 32,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Cleaning Data\n",
    "In this section, clean **all** of the issues you documented while assessing. \n",
    "\n",
    "**Note:** Make a copy of the original data before cleaning. Cleaning includes merging individual pieces of data according to the rules of [tidy data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html). The result should be a high-quality and tidy master pandas DataFrame (or DataFrames, if appropriate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies of original dataframes\n",
    "df_archive_clean = df_archive.copy()\n",
    "df_image_clean = df_image.copy()\n",
    "df_tweet_clean = df_tweet.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #1:Remove Retweets (Quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define: Remove all retweets to keep only original tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean = df_archive_clean[df_archive_clean['retweeted_status_id'].isnull()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_archive_clean['retweeted_status_id'].isnull().sum())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #2:Remove Tweets without Images (Quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "source": [
    "#### Define: Keep only tweets that contain image URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean = df_archive_clean[df_archive_clean['expanded_urls'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_archive_clean['expanded_urls'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #3: Fix Incorrect Dog Names (Quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: Replace invalid dog names like 'a', 'an', 'the', 'None', etc., with NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "invalid_names = ['a', 'an', 'the', 'None', 'very', 'just', 'my', 'like', 'unacceptable', 'officially', 'this']\n",
    "df_archive_clean['name'] = df_archive_clean['name'].apply(lambda x: np.nan if x in invalid_names else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_archive_clean['name'].value_counts().tail(10))  # Check last 10 valid dog names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #4:Convert Timestamp Column to datetime (Quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define:Convert timestamp to pandas datetime type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean['timestamp'] = pd.to_datetime(df_archive_clean['timestamp'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_archive_clean['timestamp'].dtype) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #5: Fix Data Types in tweet_json Data (Quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define:Ensure id is string and counts are integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_clean = df_tweet.copy()\n",
    "df_tweet_clean['id'] = df_tweet_clean['id'].astype(str)\n",
    "df_tweet_clean['retweet_count'] = df_tweet_clean['retweet_count'].astype(int)\n",
    "df_tweet_clean['favorite_count'] = df_tweet_clean['favorite_count'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tweet_clean.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #6:Drop Duplicate Tweet IDs (Quality)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: Remove duplicate entries by tweet ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean = df_archive_clean.drop_duplicates(subset=['tweet_id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_archive_clean['tweet_id'].duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #7:Filter Tweets Before August 1, 2017 (Quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "source": [
    "#### Define: Keep only tweets with timestamp before August 1, 2017.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean = df_archive_clean[df_archive_clean['timestamp'] < '2017-08-01']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive_clean = df_archive_clean[df_archive_clean['timestamp'] < '2017-08-01']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #8:Fix Rating Numerator and Denominator Data Types (Quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define:Ensure numerator and denominator are numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rating_numerator and rating_denominator to float\n",
    "df_archive_clean['rating_numerator'] = df_archive_clean['rating_numerator'].astype(float)\n",
    "df_archive_clean['rating_denominator'] = df_archive_clean['rating_denominator'].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_archive_clean[['rating_numerator', 'rating_denominator']].dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #9:Merge DataFrames on Tweet ID (Tidiness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define:Merge tweet archive, tweet JSON, and image predictions into one DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After splitting expanded_urls\n",
    "df_archive_clean['expanded_urls'] = df_archive_clean['expanded_urls'].astype(str)\n",
    "df_archive_clean['expanded_urls'] = df_archive_clean['expanded_urls'].str.split(',')\n",
    "\n",
    "expanded_urls_list = []\n",
    "tweet_ids_list = []\n",
    "\n",
    "for index, row in df_archive_clean.iterrows():\n",
    "    urls = row['expanded_urls']\n",
    "    if urls and len(urls) > 0:\n",
    "        for url in urls:\n",
    "            tweet_ids_list.append(row['tweet_id'])\n",
    "            expanded_urls_list.append(url)\n",
    "    else:\n",
    "        # Handle rows with empty or NaN expanded_urls\n",
    "        tweet_ids_list.append(row['tweet_id'])\n",
    "        expanded_urls_list.append(None)  # Or '' if you prefer\n",
    "\n",
    "if len(tweet_ids_list) > 0 and len(expanded_urls_list) > 0:\n",
    "    df_expanded_urls = pd.DataFrame({\n",
    "        'tweet_id': tweet_ids_list,\n",
    "        'expanded_urls': expanded_urls_list\n",
    "    })\n",
    "else:\n",
    "    # If no data, create empty DataFrame with these columns to avoid errors later\n",
    "    df_expanded_urls = pd.DataFrame(columns=['tweet_id', 'expanded_urls'])\n",
    "\n",
    "# Continue merging as before\n",
    "df_other_cols = df_archive_clean.drop(columns=['expanded_urls'])\n",
    "\n",
    "df_clean_expanded = pd.merge(df_expanded_urls, df_other_cols, on='tweet_id', how='left')\n",
    "\n",
    "print(df_clean_expanded.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Check that 'expanded_urls' column is properly exploded into multiple rows\n",
    "\n",
    "# Check how many rows have null or None expanded_urls now\n",
    "null_expanded_urls_count = df_clean_expanded['expanded_urls'].isnull().sum()\n",
    "print(f\"Number of rows with null expanded_urls: {null_expanded_urls_count}\")\n",
    "\n",
    "# Check total unique tweet_ids to make sure no duplicates lost\n",
    "unique_tweet_ids = df_clean_expanded['tweet_id'].nunique()\n",
    "print(f\"Number of unique tweet_ids after expansion: {unique_tweet_ids}\")\n",
    "\n",
    "# Display first 5 rows to verify structure and content\n",
    "print(df_clean_expanded.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #10: Inconsistent datatype of timestamp column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define:Combine doggo, floofer, pupper, puppo columns into one dog_stage column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'None' with np.nan in dog stage columns\n",
    "dog_stage_cols = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "for col in dog_stage_cols:\n",
    "    if col in df_archive.columns:\n",
    "        df_archive[col] = df_archive[col].replace('None', np.nan)\n",
    "\n",
    "# Combine columns if they exist\n",
    "if all(col in df_archive.columns for col in dog_stage_cols):\n",
    "    df_archive['dog_stage'] = df_archive[dog_stage_cols].apply(\n",
    "        lambda row: ', '.join(row.dropna()), axis=1)\n",
    "    df_archive['dog_stage'].replace('', np.nan, inplace=True)\n",
    "    df_archive['dog_stage'].replace({\n",
    "        'doggo, pupper': 'doggo and pupper',\n",
    "        'doggo, puppo': 'doggo and puppo',\n",
    "        'doggo, floofer': 'doggo and floofer'\n",
    "    }, inplace=True)\n",
    "    df_archive.drop(columns=dog_stage_cols, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_archive['dog_stage'].value_counts(dropna=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data\n",
    "Save gathered, assessed, and cleaned master dataset to a CSV file named \"twitter_archive_master.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original data\n",
    "df_archive_clean = df_archive.copy()\n",
    "df_image_clean = df_image.copy()\n",
    "df_tweet_clean = df_tweet.copy()\n",
    "\n",
    "# Cleaning (make sure to clean all issues you listed!)\n",
    "# Example: remove retweets\n",
    "df_archive_clean = df_archive_clean[df_archive_clean['retweeted_status_id'].isnull()]\n",
    "\n",
    "# Make sure tweet_id is string in all for merging\n",
    "df_archive_clean['tweet_id'] = df_archive_clean['tweet_id'].astype(str)\n",
    "df_image_clean['tweet_id'] = df_image_clean['tweet_id'].astype(str)\n",
    "df_tweet_clean['tweet_id'] = df_tweet_clean['tweet_id'].astype(str)\n",
    "\n",
    "# Merge cleaned DataFrames\n",
    "df_master = pd.merge(df_archive_clean, df_tweet_clean, on='tweet_id', how='left')\n",
    "df_master = pd.merge(df_master, df_image_clean, on='tweet_id', how='left')\n",
    "\n",
    "# Save final cleaned master dataset\n",
    "df_master.to_csv('twitter_archive_master.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Copy original data\n",
    "df_archive_clean = df_archive.copy()\n",
    "df_image_clean = df_image.copy()\n",
    "df_tweet_clean = df_tweet.copy()\n",
    "\n",
    "# Cleaning (make sure to clean all issues you listed!)\n",
    "# Example: remove retweets\n",
    "df_archive_clean = df_archive_clean[df_archive_clean['retweeted_status_id'].isnull()]\n",
    "\n",
    "# Make sure tweet_id is string in all for merging\n",
    "df_archive_clean['tweet_id'] = df_archive_clean['tweet_id'].astype(str)\n",
    "df_image_clean['tweet_id'] = df_image_clean['tweet_id'].astype(str)\n",
    "df_tweet_clean['tweet_id'] = df_tweet_clean['tweet_id'].astype(str)\n",
    "\n",
    "# Merge cleaned DataFrames\n",
    "df_master = pd.merge(df_archive_clean, df_tweet_clean, on='tweet_id', how='left')\n",
    "df_master = pd.merge(df_master, df_image_clean, on='tweet_id', how='left')\n",
    "\n",
    "# Save final cleaned master dataset\n",
    "df_master.to_csv('twitter_archive_master.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing and Visualizing Data\n",
    "In this section, analyze and visualize your wrangled data. You must produce at least **three (3) insights and one (1) visualization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = pd.read_csv('twitter_archive_master.csv')\n",
    "print(df_check.head())\n",
    "print(df_check.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "Insight 1: Tweets with the Highest Number of Favorites\n",
    "By analyzing the favorite counts, it is clear that a few tweets received exceptionally high engagement compared to others. These tweets typically contain funny or heartwarming content and often feature dogs in unique or relatable situations. High favorite counts indicate popularity and positive reception by the audience.\n",
    "\n",
    "Insight 2: Most Common Dog Stage\n",
    "Among the dog stages such as \"doggo\", \"pupper\", \"puppo\", and \"floofer\", the most frequently occurring stage was identified. \"Pupper\" appears to be the most common stage in the dataset, reflecting a general user tendency to share photos and rate younger dogs more often.\n",
    "\n",
    "Insight 3: Correlation Between Retweets and Favorites\n",
    "There is a strong positive correlation between the number of retweets and the number of favorites a tweet receives. This suggests that popular tweets not only get liked but also shared widely. Therefore, higher engagement overall is a sign of quality or viral content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_master['retweet_count'], df_master['favorite_count'], alpha=0.5)\n",
    "plt.title('Retweets vs Favorites')\n",
    "plt.xlabel('Retweet Count')\n",
    "plt.ylabel('Favorite Count')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
