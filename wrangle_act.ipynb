{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# Project: Wrangling and Analyze Data\n",
    "\n",
    "\n",
    "## About The Project\n",
    "The dataset i will be wrangling (and analyzing and visualizing) is from a tweet archive of a Twitter user with handle @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? Almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because \"they're good dogs Brent.\" WeRateDogs has over 4 million followers and has received international media coverage.\n",
    "\n",
    "WeRateDogs downloaded their Twitter archive and sent it to Udacity via email exclusively for Udacity students to use in their project. This archive contains basic tweet data (tweet ID, timestamp, text, etc.) for all 5000+ of their tweets as they stood on August 1, 2017.\n",
    "\n",
    "My project entails the following majorsteps / tasks with sub tasks to give it for details:\n",
    "\n",
    "Step 1: Gathering data\n",
    "\n",
    "Step 2: Assessing data\n",
    "\n",
    "Step 3: Cleaning data\n",
    "\n",
    "Step 4: Storing data\n",
    "\n",
    "Step 5: Analyzing, and visualizing data\n",
    "\n",
    "Step 6: Reporting\n",
    "\n",
    "your data wrangling efforts\n",
    "your data analyses and visualizations\n",
    "\n",
    "\n",
    "I will be importing the following modules for my project :\n",
    "\n",
    "1.pandas\n",
    "2.NumPy\n",
    "3.requests\n",
    "4.tweepy\n",
    "5.json\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Project's Dataset\n",
    "\n",
    "I will be working with three (3) datasets. Below is a detailed description of the datasets i will be working with throughout the project. From these datasets i will provide valuable insights\n",
    "\n",
    "***1. Enhanced Twitter Archive***\n",
    "\n",
    "The WeRateDogs Twitter archive contains basic tweet data for all 5000+ of their tweets, but not everything. One column the archive does contain though: each tweet's text, which I used to extract rating, dog name, and dog \"stage\" (i.e. doggo, floofer, pupper, and puppo) to make this Twitter archive \"enhanced.\" Of the 5000+ tweets, I have filtered for tweets with ratings only (there are 2356).\n",
    "\n",
    "\n",
    "****2. Additional Data via the Twitter API***\n",
    "\n",
    "Back to the basic-ness of Twitter archives: retweet count and favorite count are two of the notable column omissions. Fortunately, this additional data can be gathered by anyone from Twitter's API. Well, \"anyone\" who has access to data for the 3000 most recent tweets, at least. \n",
    "\n",
    "\n",
    "***3.Image Predictions File***\n",
    "\n",
    "One more cool thing: I ran every image in the WeRateDogs Twitter archive through a neural network that can classify breeds of dogs*. The results: a table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#Loading modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "import tweepy\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "In the cell below, gather **all** three pieces of data for this project and load them in the notebook. **Note:** the methods required to gather each data are different.\n",
    "1. Directly download the WeRateDogs Twitter archive data (twitter_archive_enhanced.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading The Enhanced Twitter Archive csv\n",
    "\n",
    "twitter_archive = pd.read_csv('twitter-archive-enhanced.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a look at the uploaded data\n",
    "\n",
    "twitter_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the Requests library to download the tweet image prediction (image_predictions.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Opening a tsv file and saving the response content\n",
    "with open('image-predictions.tsv', mode='wb') as file:\n",
    "    file.write(response.content)\n",
    "    \n",
    "# Read TSV file\n",
    "image_prediction = pd.read_csv('image-predictions.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the information in our data\n",
    "image_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the Tweepy library to query additional data via the Twitter API (tweet_json.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Twitter API for each tweet in the Twitter archive and save JSON in a text file\n",
    "# These are hidden to comply with Twitter's API terms and conditions\n",
    "\n",
    "# Get the API object which we will use to gather the twitter data\n",
    "import tweepy\n",
    "\n",
    "consumer_key = 'Secret'\n",
    "consumer_secret = 'Secret'\n",
    "access_token = 'Secret'\n",
    "access_secret = 'Secret'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exrtract the IDs from twitter_archive\n",
    "twitter_archive.tweet_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Express the tweet IDs as a list\n",
    "list(twitter_archive.tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the total numbers of tweet IDs\n",
    "len(list(twitter_archive.tweet_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using one tweet ID as an example: Get the status of one tweet ID\n",
    "page = api.get_status(891815181378084864, tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet IDs for which to gather additional data via Twitter's API\n",
    "tweet_ids = twitter_archive.tweet_id.values\n",
    "tweet_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Twitter's API for the JSON data of each tweet ID in the Twitter archive\n",
    "index = 0\n",
    "# dictionary to catch the errors\n",
    "error_dict = {}\n",
    "start = time.time()\n",
    "\n",
    "# Save each tweet's returned JSON as a new line in a .txt file\n",
    "with open ('tweet_json.txt', 'w') as tweet_bk:\n",
    "    # This will likely take 20 - 30 minutes to run because of Twitter's rate limit\n",
    "    for tweet_id in tweet_ids:\n",
    "        index += 1\n",
    "        try:\n",
    "            # Get the status data for each of the tweet IDs\n",
    "            tweet = api.get_status(tweet_id, tweet_mode = 'extended')\n",
    "            print(str(index) + \": \" + \"ID - \" + str(tweet_id))\n",
    "            # Convert each tweet status to JSON string and save it in the tweet_bk file\n",
    "            json.dump(tweet._json, tweet_bk)\n",
    "            # recognize \\n as a break of text\n",
    "            tweet_bk.write(\"\\n\")\n",
    "            \n",
    "        # Catching errors that might occur while accessing the tweet data or content\n",
    "        except tweepy.TweepyException as error:\n",
    "            print(str(index) + \": \" + \"ID - \" + str(tweet_id) + \" has an error:\", error.response.text)\n",
    "            # Appending the errors to the dictionary; error_dict\n",
    "            error_dict[tweet_id] = error\n",
    "            \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the missing columns from enhanced twitter archive\n",
    "\n",
    "# Empty list to convert to DataFrame\n",
    "df_list = []\n",
    "\n",
    "# Open text file for reading\n",
    "with open ('tweet_json.txt', 'r') as json_file:\n",
    "    for line in json_file.readlines():\n",
    "        # Read each JSON string status into a dictionary and reading each line as a dictionary\n",
    "        each_tweet_line = json.loads(line)\n",
    "        # Getting the required parameters\n",
    "        tweet_id = each_tweet_line['id']\n",
    "        retwt_count = each_tweet_line['retweet_count']\n",
    "        fav_count = each_tweet_line['favorite_count']\n",
    "        follows_count = each_tweet_line['user']['followers_count']\n",
    "        frnds_count = each_tweet_line['user']['friends_count']\n",
    "        \n",
    "        df_list.append({'id': tweet_id,\n",
    "                       'retweet_count': retwt_count,\n",
    "                       'favorite_count': fav_count,\n",
    "                       'followers_count': follows_count,\n",
    "                       'friends_count': frnds_count})\n",
    "        \n",
    "tweet_json = pd.DataFrame(df_list, columns=['id', 'retweet_count', 'favorite_count', 'followers_count', 'friends_count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 28,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Assessing Data\n",
    "In this section, detect and document at least **eight (8) quality issues and two (2) tidiness issue**. You must use **both** visual assessment\n",
    "programmatic assessement to assess the data.\n",
    "\n",
    "**Note:** pay attention to the following key points when you access the data.\n",
    "\n",
    "* You only want original ratings (no retweets) that have images. Though there are 5000+ tweets in the dataset, not all are dog ratings and some are retweets.\n",
    "* Assessing and cleaning the entire dataset completely would require a lot of time, and is not necessary to practice and demonstrate your skills in data wrangling. Therefore, the requirements of this project are only to assess and clean at least 8 quality issues and at least 2 tidiness issues in this dataset.\n",
    "* The fact that the rating numerators are greater than the denominators does not need to be cleaned. This [unique rating system](http://knowyourmeme.com/memes/theyre-good-dogs-brent) is a big part of the popularity of WeRateDogs.\n",
    "* You do not need to gather the tweets beyond August 1st, 2017. You can, but note that you won't be able to gather the image predictions for these tweets since you don't have access to the algorithm used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual Assessment\n",
    "\n",
    "\n",
    "Each piece of gathered data is displayed in the Jupyter Notebook for visual assesment purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = twitter_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a look at the information in our Enhanced_Twit_arc data\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = image_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a look at the information in our image_prediction data\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = tweet_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the information in our tweet_json data\n",
    "df3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmatic Assessment\n",
    "\n",
    "\n",
    "Using pandas' functions and/or methods to assess each gathered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the concise summary of our Enhanced_Twit_arc data\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the concise summary of our image_prediction data\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the concise summary of our tweet_json data\n",
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the statistical description of our Enhanced_Twit_arc data\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the statistical description of our image_prediction data\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the statistical description of our tweet_json data\n",
    "df3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at a few rows of our Enhanced_Twit_arc data data to check out relevant issues\n",
    "df1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at a few rows of our image_prediction data to check out relevant issues\n",
    "df2.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at a few rows of our tweet_json data to check out relevant issues\n",
    "df3.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows and columns in image_prediction dataframe\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows and columns in tweet_json dataframe\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of missing values in our Enhanced_Twit_arc data\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of missing values in our image_prediction data\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of missing values in our tweet_json data\n",
    "df3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numbe rof unique values in the columns in our twitter_archive data\n",
    "df1.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique values in the columns in our image_prediction data\n",
    "df2.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique values in the columns in our tweet_json data\n",
    "df3.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of duplicate rows in our Enhanced_Twit_arc data\n",
    "df1.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of duplicate rows in our image_prediction data\n",
    "df2.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of duplicate rows in our tweet_json data\n",
    "df3.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the name of the columns\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for IDs with values in retweet_status_id, retweeted_status_user_id, and\n",
    "# retweeted_status_timestamp columns\n",
    "df1[df1['retweeted_status_id'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rating_numerator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rating_denominator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality issues\n",
    "\n",
    "Enhanced_Twit_arc (df1)\n",
    "\n",
    "There are tweet IDs that have 'retweeted_status_id, retweeted_status_user _id, and retweeted_status _timestamp values. These ids are that of retweets and won't be used for our analysis\n",
    "\n",
    "retweeted_status_id, retweeted_status_user_id, and retweeted_status_timestamp columns contain mostly missing values\n",
    "\n",
    "in_reply_to_status_id and in_reply_to_user_id columns contain mostly missing values\n",
    "\n",
    "missing values in expanded_urls column\n",
    "\n",
    "Timestamp column is in int instead of datetime data type\n",
    "\n",
    "Tweet id column is in int instead of string data type\n",
    "\n",
    "\n",
    "**Image_predictions (df2)**\n",
    "\n",
    "The tweet id column is in int instead of string data type\n",
    "Values in columns 'p1', 'p2', and 'p3' don't have consistent format\n",
    "\n",
    "\n",
    "**tweet_json (df3)**\n",
    "\n",
    "\n",
    "Erroneous data type (tweet id column is in int instead of string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 7,
        "hidden": false,
        "row": 40,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "### Tidiness issues\n",
    "\n",
    "\n",
    "**General**\n",
    "The column label for tweet IDs should be the same across the three datasets.\n",
    "\n",
    "\n",
    "**twitter_archive (df1)**\n",
    "4 columns (doggo, floofer, pupper, puppo) are categories of dog 'stage' and need to be one column 'stage' with 4 categories: doggo, floofer, pupper and puppo in it.\n",
    "\n",
    "\n",
    "**tweet_json (df3)**\n",
    "followers_count column has only 24 values and and friends_count columns contains only 1 value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 32,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Cleaning Data\n",
    "In this section, clean **all** of the issues you documented while assessing. \n",
    "\n",
    "**Note:** Make a copy of the original data before cleaning. Cleaning includes merging individual pieces of data according to the rules of [tidy data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html). The result should be a high-quality and tidy master pandas DataFrame (or DataFrames, if appropriate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies of original pieces of data\n",
    "df1_clean = df1.copy()\n",
    "df2_clean = df2.copy()\n",
    "df3_clean = df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #1: Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: \n",
    "\n",
    "Remove tweet IDs that have retweeted. These ids are that of retweets and won't be used for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop retweeted rows\n",
    "df1_clean = df1_clean[df1_clean.retweeted_status_id.isnull()]\n",
    "df1_clean = df1_clean[df1_clean.retweeted_status_user_id.isnull()]\n",
    "df1_clean = df1_clean[df1_clean.retweeted_status_timestamp.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the retweets have been droped\n",
    "print(df1_clean.retweeted_status_id.notnull().sum())\n",
    "print(df1_clean.retweeted_status_user_id.notnull().sum())\n",
    "print(df1_clean.retweeted_status_timestamp.notnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #2:\n",
    "\n",
    "Followers_count column has only 24 values and friends_count columns contains only 1 value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "source": [
    "#### Define\n",
    "\n",
    "Drop followers_count and friends_count columns as they don't contain necessary values that would be relevant to the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_clean.drop(['followers_count', 'friends_count'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some dog names are invalid eg None, a, an*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define**\n",
    "\n",
    "Convert invalid names (None or starting wih lower case letters) to NaN and extract the correct names from the text column after the word \"named\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_t_archive.name = clean_t_archive.name.replace(regex=['^[a-z]+', 'None'], value =np.nan)\n",
    "\n",
    "# Checking the number of  null values in name column after conversion\n",
    "sum(clean_t_archive.name.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to extract names from text column, and return NaN if there is no 'named' word\n",
    "\n",
    "def function(text):\n",
    "    txt_list = text.split()\n",
    "    for word in txt_list:\n",
    "        if word.lower() == 'named':\n",
    "            name_index = txt_list.index(word) + 1\n",
    "            return txt_list[name_index]\n",
    "        else:\n",
    "            pass\n",
    "    return np.nan\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data\n",
    "Save gathered, assessed, and cleaned master dataset to a CSV file named \"twitter_archive_master.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the master dataset to a csv file\n",
    "data.to_csv(\"twitter_archive_master.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it was properly saved\n",
    "data = pd.read_csv(\"twitter_archive_master.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing and Visualizing Data\n",
    "In this section, analyze and visualize your wrangled data. You must produce at least **three (3) insights and one (1) visualization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the description of our master dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean value of the dog names\n",
    "data.name.value_counts() / data.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "1.The minimum retweet count is 11, mean is 2245, and the maximum retweet count is 70786\n",
    "\n",
    "2.Image number 1 is the most prominent (frequent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Does retweet count positively correlate with favourite count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: How many image number occured most for each tweet's most confident image prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value count of each image number value\n",
    "data.img_num.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use countplot to get the distribution of the most frequent image number that corresponds \n",
    "# to the most confident prediction\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.countplot(data.img_num)\n",
    "plt.title('The Distribution of Tweet Image Number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: What is the most popular dog stage according to the neural network's image prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = 'darkgrid')\n",
    "sorted_age = data['stage'].value_counts().head(3).index\n",
    "sns.countplot(data = data, x ='stage', order = sorted_age, orient='h')\n",
    "plt.xlabel('Dog stages', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.title('The Distribution of Dog Stages', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('dark')\n",
    "sns.regplot(data.retweet_count, data.favorite_count, scatter_kws = {'color': 'Green'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value count of each dog stage\n",
    "data.stage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFERENCES :\n",
    "\n",
    "https://stackoverflow.com/questions/31431002/unable-to-import-tweepy-module\n",
    "\n",
    "https://stackoverflow.com/questions/57062501/i-cant-install-json-module\n",
    "\n",
    "https://medium.com/@chisompromise/twitter-data-analysis-weratedogs-1fb8b65da7fa\n",
    "\n",
    "https://m.youtube.com/watch?v=0dkzcshJz0k"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
