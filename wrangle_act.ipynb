{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Wrangle and Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Real-world data rarely comes clean. Using Python and its libraries, you will gather data from a variety of sources and in a variety of formats, assess its quality and tidiness, then clean it. This is called data wrangling. You will document your wrangling efforts in a Jupyter Notebook, plus showcase them through analyses and visualizations using Python (and its libraries) and/or SQL.\n",
    "\n",
    "The dataset that you will be wrangling (and analyzing and visualizing) is the tweet archive of Twitter user [@dog_rates](https://twitter.com/dog_rates), also known as [WeRateDogs](https://en.wikipedia.org/wiki/WeRateDogs). WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? Almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because \"[they're good dogs Brent](http://knowyourmeme.com/memes/theyre-good-dogs-brent).\" WeRateDogs has over 4 million followers and has received international media coverage.\n",
    "\n",
    "WeRateDogs [downloaded their Twitter archive](https://support.twitter.com/articles/20170160) and sent it to Udacity via email exclusively for you to use in this project. This archive contains basic tweet data (tweet ID, timestamp, text, etc.) for all 5000+ of their tweets as they stood on August 1, 2017. More on this soon.\n",
    "\n",
    "<img src = \"https://video.udacity-data.com/topher/2017/October/59dd378f_dog-rates-social/dog-rates-social.jpg\" height =200px, width = 400px>\n",
    "<center>Image via <a href = \"http://www.bostonmagazine.com/arts-entertainment/blog/2017/04/18/dog-rates-mit/\">Boston Magazine</a></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Software Do I Need?\n",
    "\n",
    "The entirety of this project can be completed inside the Udacity classroom on the **Project Workspace: Complete and Submit Project** page using the Jupyter Notebook provided there. (Note: This Project Workspace may not be available in all versions of this project, in which case you should follow the directions below.)\n",
    "\n",
    "If you want to work outside of the Udacity classroom, the following software requirements apply:\n",
    "\n",
    "* You need to be able to work in a Jupyter Notebook on your computer. Please revisit our Jupyter Notebook and Anaconda tutorials earlier in the Nanodegree program for installation instructions.\n",
    "* The following packages (libraries) need to be installed. You can install these packages via conda or pip. Please revisit our Anaconda tutorial earlier in the Nanodegree program for package installation instructions.\n",
    "  * pandas\n",
    "  * NumPy\n",
    "  * requests\n",
    "  * tweepy\n",
    "  * json\n",
    "* You need to be able to create written documents that contain images and you need to be able to export these documents as PDF files. This task can be done in a Jupyter Notebook, but you might prefer to use a word processor like [Google Docs](https://www.google.com/docs/about/), which is free, or Microsoft Word.\n",
    "* A text editor, like [Sublime](https://www.sublimetext.com/), which is free, will be useful but is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Motivation\n",
    "\n",
    "### Context\n",
    "\n",
    "Your goal: wrangle WeRateDogs Twitter data to create interesting and trustworthy analyses and visualizations. The Twitter archive is great, but it only contains very basic tweet information. Additional gathering, then assessing and cleaning is required for \"Wow!\"-worthy analyses and visualizations.\n",
    "\n",
    "### The Data\n",
    "\n",
    "### Enhanced Twitter Archive\n",
    "\n",
    "The WeRateDogs Twitter archive contains basic tweet data for all 5000+ of their tweets, but not everything. One column the archive does contain though: each tweet's text, which I used to extract rating, dog name, and dog \"stage\" (i.e. doggo, floofer, pupper, and puppo) to make this Twitter archive \"enhanced.\" Of the 5000+ tweets, I have filtered for tweets with ratings only (there are 2356).\n",
    "Extracted data from tweet text\n",
    "\n",
    "<img src = \"https://video.udacity-data.com/topher/2017/October/59dd4791_screenshot-2017-10-10-18.19.36/screenshot-2017-10-10-18.19.36.png\" height = 400px, width= 800px>\n",
    "\n",
    "<center><em><strong>The extracted data from each tweet's text</strong></em></center>\n",
    "\n",
    "I extracted this data programmatically, but I didn't do a very good job. The ratings probably aren't all correct. Same goes for the dog names and probably dog stages (see below for more information on these) too. You'll need to assess and clean these columns if you want to use them for analysis and visualization.\n",
    "\n",
    "<img src=\"https://video.udacity-data.com/topher/2017/October/59e04ceb_dogtionary-combined/dogtionary-combined.png\" height = 400px, width=900px>\n",
    "\n",
    "<center><em><strong>The Dogtionary explains the various stages of dog: doggo, pupper, puppo, and floof(er) (via the <a href = \"https://www.amazon.com/WeRateDogs-Most-Hilarious-Adorable-Youve/dp/1510717145\">#WeRateDogs</a> book on Amazon)</strong></em></center>\n",
    "\n",
    "### Additional Data via the Twitter API\n",
    "\n",
    "Back to the basic-ness of Twitter archives: retweet count and favorite count are two of the notable column omissions. Fortunately, this additional data can be gathered by anyone from Twitter's API. Well, \"anyone\" who has access to data for the 3000 most recent tweets, at least. But you, because you have the WeRateDogs Twitter archive and specifically the tweet IDs within it, can gather this data for all 5000+. And guess what? You're going to query Twitter's API to gather this valuable data.\n",
    "\n",
    "### Image Predictions File\n",
    "\n",
    "One more cool thing: I ran every image in the WeRateDogs Twitter archive through a [neural network](https://www.youtube.com/watch?v=2-Ol7ZB0MmU) that can classify breeds of dogs*. The results: a table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images).\n",
    "\n",
    "<img src=\"https://video.udacity-data.com/topher/2017/October/59dd4d2c_screenshot-2017-10-10-18.43.41/screenshot-2017-10-10-18.43.41.png\" width = 900px, height=400px>\n",
    "<center><em><strong>Tweet image prediction data</strong></em></center>\n",
    "\n",
    "So for the last row in that table:\n",
    "\n",
    "* tweet_id is the last part of the tweet URL after \"status/\" → https://twitter.com/dog_rates/status/889531135344209921\n",
    "* p1 is the algorithm's #1 prediction for the image in the tweet → <strong>golden retriever</strong>\n",
    "* p1_conf is how confident the algorithm is in its #1 prediction → <strong>95%</strong>\n",
    "* p1_dog is whether or not the #1 prediction is a breed of dog → <strong>TRUE</strong>\n",
    "* p2 is the algorithm's second most likely prediction → Labrador <strong>retriever</strong>\n",
    "* p2_conf is how confident the algorithm is in its #2 prediction → <strong>1%</strong>\n",
    "* p2_dog is whether or not the #2 prediction is a breed of dog → <strong>TRUE</strong>\n",
    "* etc.\n",
    "\n",
    "And the #1 prediction for the image in that tweet was spot on:\n",
    "\n",
    "<img src=\"https://video.udacity-data.com/topher/2017/October/59dd4e05_dog-pred/dog-pred.png\" width = 250px, height=200px>\n",
    "<center><strong>A golden retriever named Stuart</strong></center>\n",
    "\n",
    "So that's all fun and good. But all of this additional data will need to be gathered, assessed, and cleaned. This is where you come in.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "Key points to keep in mind when data wrangling for this project:\n",
    "<ul>\n",
    "    <li>You only want original ratings (no retweets) that have images. Though there are 5000+ tweets in the dataset, not all are dog ratings and some are retweets.</li>\n",
    "    <li>Assessing and cleaning the entire dataset completely would require a lot of time, and is not necessary to practice and demonstrate your skills in data wrangling. Therefore, the requirements of this project are only to assess and clean at least 8 quality issues and at least 2 tidiness issues in this dataset.</li>\n",
    "    <li>Cleaning includes merging individual pieces of data according to the rules of <a href=\"https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html\">tidy data</a>.</li>\n",
    "    <li>The fact that the rating numerators are greater than the denominators does not need to be cleaned. This <a href=\"http://knowyourmeme.com/memes/theyre-good-dogs-brent\">unique rating system</a> is a big part of the popularity of WeRateDogs.</li>\n",
    "    <li>You do not need to gather the tweets beyond August 1st, 2017. You can, but note that you won't be able to gather the image predictions for these tweets since you don't have access to the algorithm used.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Details\n",
    "\n",
    "Your tasks in this project are as follows:\n",
    "<ul>\n",
    "    <li>Data wrangling, which consists of:</li>\n",
    "        <ul>\n",
    "          <li>Gathering data (downloadable file in the Resources tab in the left most panel of your classroom and linked in step 1 below).</li>\n",
    "           <li>Assessing data</li>\n",
    "           <li>Cleaning data</li>\n",
    "         </ul>\n",
    "    <li>Storing, analyzing, and visualizing your wrangled data</li>\n",
    "    <li>Reporting on \n",
    "        1) your data wrangling efforts and \n",
    "        2) your data analyses and visualizations</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jupyterthemes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b8e3eee4bb22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjupyterthemes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjtplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mjtplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'onedork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jupyterthemes'"
     ]
    }
   ],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import tweepy\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from IPython.display import Image\n",
    "from functools import reduce\n",
    "import re\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='onedork')\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyterthemes==0.16.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the csv file\n",
    "df_twitter_archive = pd.read_csv('twitter-archive-enhanced-2.csv')\n",
    "df_twitter_archive.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tweet image prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the image prediction file using the link provided to Udacity students\n",
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "image_request = requests.get(url, allow_redirects=True)\n",
    "\n",
    "open('image_predictions.tsv', 'wb').write(image_request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the data in the image predictions file\n",
    "df_image_predictions = pd.read_csv('image_predictions.tsv', sep = '\\t')\n",
    "df_image_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ref: https://stackoverflow.com/questions/28384588/twitter-api-get-tweets-with-specific-id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler('5Uur0mo4ol2kB8yhtZ1VxXS0u', 'h8E7fSpXWiMoBel7G1ZOAeu4Mgru0v0MtxH5ehYE1RKM89SiBH')\n",
    "auth.set_access_token('303562412-ct9aNnU0FQR0UKJVn1i1W3Y8omqSewiQWUcRaygB', 'D3qslrbdOU5fqTOp951kOIuZbkeTPBodnjNYoEGFR63Ft')\n",
    "api = tweepy.API(auth, \n",
    "                 parser = tweepy.parsers.JSONParser(), \n",
    "                 wait_on_rate_limit = True, \n",
    "                 wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter API & JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Tweepy status object based on Tweet ID and store in list\n",
    "list_of_tweets = []\n",
    "# Tweets that can't be found are saved in the list below:\n",
    "cant_find_tweets_for_those_ids = []\n",
    "for tweet_id in df_twitter_archive['tweet_id']:   \n",
    "    try:\n",
    "        list_of_tweets.append(api.get_status(tweet_id))\n",
    "    except Exception as e:\n",
    "        cant_find_tweets_for_those_ids.append(tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing \n",
    "print(\"The list of tweets\" ,len(list_of_tweets))\n",
    "print(\"The list of tweets no found\" , len(cant_find_tweets_for_those_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then in this code block we isolate the json part of each tweepy \n",
    "#status object that we have downloaded and we add them all into a list\n",
    "my_list_of_dicts = []\n",
    "for each_json_tweet in list_of_tweets:\n",
    "    my_list_of_dicts.append(each_json_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we write this list into a txt file:\n",
    "with open('tweet_json.txt', 'w') as file:\n",
    "        file.write(json.dumps(my_list_of_dicts, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify information of interest from JSON dictionaries in txt file\n",
    "#and put it in a dataframe called tweet JSON\n",
    "my_demo_list = []\n",
    "with open('tweet_json.txt', encoding='utf-8') as json_file:  \n",
    "    all_data = json.load(json_file)\n",
    "    for each_dictionary in all_data:\n",
    "        tweet_id = each_dictionary['id']\n",
    "        whole_tweet = each_dictionary['text']\n",
    "        only_url = whole_tweet[whole_tweet.find('https'):]\n",
    "        favorite_count = each_dictionary['favorite_count']\n",
    "        retweet_count = each_dictionary['retweet_count']\n",
    "        followers_count = each_dictionary['user']['followers_count']\n",
    "        friends_count = each_dictionary['user']['friends_count']\n",
    "        whole_source = each_dictionary['source']\n",
    "        only_device = whole_source[whole_source.find('rel=\"nofollow\">') + 15:-4]\n",
    "        source = only_device\n",
    "        retweeted_status = each_dictionary['retweeted_status'] = each_dictionary.get('retweeted_status', 'Original tweet')\n",
    "        if retweeted_status == 'Original tweet':\n",
    "            url = only_url\n",
    "        else:\n",
    "            retweeted_status = 'This is a retweet'\n",
    "            url = 'This is a retweet'\n",
    "\n",
    "        my_demo_list.append({'tweet_id': str(tweet_id),\n",
    "                             'favorite_count': int(favorite_count),\n",
    "                             'retweet_count': int(retweet_count),\n",
    "                             'followers_count': int(followers_count),\n",
    "                             'friends_count': int(friends_count),\n",
    "                             'url': url,\n",
    "                             'source': source,\n",
    "                             'retweeted_status': retweeted_status,\n",
    "                            })\n",
    "        tweet_json = pd.DataFrame(my_demo_list, columns = ['tweet_id', 'favorite_count','retweet_count', \n",
    "                                                           'followers_count', 'friends_count','source', \n",
    "                                                           'retweeted_status', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (**Visual assessment**)  Each piece of gathered data is displayed in the Jupyter Notebook for visual assessment purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (**Programmatic assessment**) Pandas' functions and/or methods are used to assess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_predictions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Archive Dataframe Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive.rating_numerator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_twitter_archive.loc[df_twitter_archive.rating_numerator == 204, 'text']) \n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_numerator == 143, 'text']) \n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_numerator == 666, 'text']) \n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_numerator == 1176, 'text'])\n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_numerator == 144, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print whole text in order to verify numerators and denominators\n",
    "#17 dogs\n",
    "print(df_twitter_archive['text'][1120]) \n",
    "#13 dogs\n",
    "print(df_twitter_archive['text'][1634])\n",
    "#just a tweet to explain actual ratings, this will be ignored when cleaning data\n",
    "print(df_twitter_archive['text'][313]) \n",
    "#no picture, this will be ignored when cleaning data\n",
    "print(df_twitter_archive['text'][189]) \n",
    "#12 dogs\n",
    "print(df_twitter_archive['text'][1779]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive.rating_denominator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_twitter_archive.loc[df_twitter_archive.rating_denominator == 11, 'text']) \n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_denominator == 2, 'text']) \n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_denominator == 16, 'text']) \n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_denominator == 15, 'text'])\n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_denominator == 7, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retweet - it will be deleted when delete all retweets\n",
    "print(df_twitter_archive['text'][784]) \n",
    "#actual rating 14/10 need to change manually\n",
    "print(df_twitter_archive['text'][1068]) \n",
    "#actual rating 10/10 need to change manually\n",
    "print(df_twitter_archive['text'][1662]) \n",
    "#actual rating 9/10 need to change manually\n",
    "print(df_twitter_archive['text'][2335]) \n",
    "#tweet to explain rating\n",
    "print(df_twitter_archive['text'][1663]) \n",
    "#no rating - delete\n",
    "print(df_twitter_archive['text'][342]) \n",
    "#no rating - delete\n",
    "print(df_twitter_archive['text'][516]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive[df_twitter_archive.tweet_id.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Dataframe Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_predictions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an image for tweet_id 856282028240666624\n",
    "Image(url = 'https://pbs.twimg.com/media/C-If9ZwXoAAfDX2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_predictions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_predictions[df_image_predictions.tweet_id.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_predictions['p1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_predictions['p2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_predictions['p3'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter Counts Dataframe Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean\n",
    "\n",
    "This section consists of the cleaning portion of the data wrangling process:\n",
    "\n",
    "* Define\n",
    "* Code\n",
    "* Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the tables before cleaning\n",
    "df_twitter_archive_clean = df_twitter_archive.copy()\n",
    "df_image_predictions_clean = df_image_predictions.copy()\n",
    "tweet_json_clean = tweet_json.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "\n",
    "1. Merge the `clean versions` of `df_twitter_archive`, `df_image_predictions`, and `tweet_json` dataframes Correct the dog types\n",
    "2. Create one column for the various dog types: doggo, floofer, pupper, puppo Remove columns no longer needed: in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, and retweeted_status_timestamp\n",
    "3. Delete retweets\n",
    "4. Remove columns no longer needed\n",
    "5. Change tweet_id from an integer to a string\n",
    "6. Change the timestamp to correct datetime format\n",
    "7. Correct naming issues\n",
    "8. Standardize dog ratings\n",
    "9. Creating a new dog_breed column using the image prediction data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Merge the clean versions of df_twitter_archive, df_image_predictions, and tweet_json dataframes Correct the dog types\n",
    "\n",
    "**Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://stackoverflow.com/questions/44327999/python-pandas-merge-multiple-dataframes/44338256\n",
    "dfs = pd.concat([df_twitter_archive_clean, df_image_predictions_clean, tweet_json_clean], join='outer', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Code and Test**: Create one column for the various dog types: doggo, floofer, pupper, puppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text from the columns into the new dog_type colunn\n",
    "dfs['dog_type'] = dfs['text'].str.extract('(doggo|floofer|pupper|puppo)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[['dog_type', 'doggo', 'floofer', 'pupper', 'puppo']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.dog_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Code and Test**: Delete retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dfs[np.isnan(dfs.retweeted_status_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify no non-null entires are left\n",
    "dfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the following columns:\n",
    "dfs = dfs.drop(['retweeted_status_id', \\\n",
    "                                  'retweeted_status_user_id', 'retweeted_status_timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  **Code and Test**: Remove columns no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.drop(['in_reply_to_status_id', \n",
    "                  'in_reply_to_user_id',\n",
    "                  'source',\n",
    "                  'img_num',\n",
    "                  'friends_count',\n",
    "                  'source',\n",
    "                  'url',\n",
    "                  'followers_count'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://stackoverflow.com/questions/14984119/python-pandas-remove-duplicate-columns\n",
    "dfs = dfs.loc[:,~dfs.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.drop(['retweeted_status'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Code and Test**: Change tweet_id from an integer to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['tweet_id'] = dfs['tweet_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * **Code and Test**: Timestamps to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the time zone from the 'timestamp' column\n",
    "dfs['timestamp'] = dfs['timestamp'].str.slice(start=0, stop=-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the 'timestamp' column to a datetime object\n",
    "dfs['timestamp'] = pd.to_datetime(dfs['timestamp'], format = \"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Code and Test**: Correct naming issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.name = dfs.name.str.replace('^[a-z]+', 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['name'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Code and Test**: Standardize dog ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['rating_numerator'] = dfs['rating_numerator'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['rating_denominator'] = dfs['rating_denominator'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to gather all text, indices, and ratings for tweets that contain a decimal in the numerator of the rating\n",
    "ratings_decimals_text = []\n",
    "ratings_decimals_index = []\n",
    "ratings_decimals = []\n",
    "\n",
    "for i, text in dfs['text'].iteritems():\n",
    "    if bool(re.search('\\d+\\.\\d+\\/\\d+', text)):\n",
    "        ratings_decimals_text.append(text)\n",
    "        ratings_decimals_index.append(i)\n",
    "        ratings_decimals.append(re.search('\\d+\\.\\d+', text).group())\n",
    "\n",
    "# Print ratings with decimals        \n",
    "ratings_decimals_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the indices of the ratings above (have decimal)\n",
    "ratings_decimals_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correctly converting the above decimal ratings to float\n",
    "dfs.loc[ratings_decimals_index[0],'rating_numerator'] = float(ratings_decimals[0])\n",
    "dfs.loc[ratings_decimals_index[1],'rating_numerator'] = float(ratings_decimals[1])\n",
    "dfs.loc[ratings_decimals_index[2],'rating_numerator'] = float(ratings_decimals[2])\n",
    "dfs.loc[ratings_decimals_index[3],'rating_numerator'] = float(ratings_decimals[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the indices \n",
    "dfs.loc[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url = 'https://pbs.twimg.com/media/CUCQTpEWEAA7EDz.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column called rating, and calulate the value with new, standardized ratings\n",
    "dfs['rating'] = dfs['rating_numerator'] / dfs['rating_denominator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.loc[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Clean and Test**: Creating a new dog_breed column using the image prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['dog_breed'] = 'None'\n",
    "\n",
    "for i, row in dfs.iterrows():\n",
    "\n",
    "    if row.p1_dog:\n",
    "        dfs.set_value(i, 'dog_breed', row.p1)\n",
    "    elif row.p2_dog and row.rating_numerator >= 10:\n",
    "        dfs.set_value(i, 'dog_breed', row.p2)\n",
    "    elif row.p3_dog and row.rating_numerator >= 10:\n",
    "        dfs.set_value(i, 'dog_breed', row.p3)\n",
    "    else:\n",
    "        dfs.set_value(i, 'dog_breed', 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.dog_breed.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing, Analyzing, and Visualizing Data\n",
    "\n",
    "This section provides an analysis of the data set, and corresponding visualizations to draw valuable conclusions.\n",
    "\n",
    "   1. Visualizing the total number of tweets over time to see whether that number increases, or decreases, over time.\n",
    "   2. Visualizing the retweet counts, and favorite counts comparison over time.\n",
    "   3. Visualizing the most popular dog breed\n",
    "   4. Visualizing the most popular dog names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the new twitter_dogs df to a new csv file\n",
    "dfs.to_csv('twitter_archive_master.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Analyze and Visualize**: Visualizing the total number of tweets over time to see whether that number increases, or decreases, over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.timestamp = pd.to_datetime(dfs['timestamp'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "monthly_tweets = dfs.groupby(pd.Grouper(key = 'timestamp', freq = \"M\")).count().reset_index()\n",
    "monthly_tweets = monthly_tweets[['timestamp', 'tweet_id']]\n",
    "monthly_tweets.head()\n",
    "monthly_tweets.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting time vs. tweets\n",
    "\n",
    "plt.figure(figsize=(10, 10));\n",
    "plt.xlim([datetime.date(2015, 11, 30), datetime.date(2017, 7, 30)]);\n",
    "\n",
    "plt.xlabel('Year and Month')\n",
    "plt.ylabel('Tweets Count')\n",
    "\n",
    "plt.plot(monthly_tweets.timestamp, monthly_tweets.tweet_id);\n",
    "plt.title('We Rate Dogs Tweets over Time');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over time tweets decreased sharply, with spikes in activity during the early  of 2016(Jan), 2016(Mar), and generally decreasing from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Analyze and Visualize**: Visualizing the retweet counts, and favorite counts comparison over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of retweets vs favorite count\n",
    "\n",
    "sns.lmplot(x=\"retweet_count\", \n",
    "           y=\"favorite_count\", \n",
    "           data=dfs,\n",
    "           size = 5,\n",
    "           aspect=1.3,\n",
    "           scatter_kws={'alpha':1/5});\n",
    "\n",
    "plt.title('Favorite Count vs. Retweet Count');\n",
    "plt.xlabel('Retweet Count');\n",
    "plt.ylabel('Favorite Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Favorite counts are correlated with retweet counts - this is a positive correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Analyze and Visualize**: Visualizing the most popular dog breed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['dog_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most popular dog breed is a golden retriever, with a labrador retriever coming in as the second most popular breed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram to visualize dog breeeds\n",
    "dog_breed = dfs.groupby('dog_breed').filter(lambda x: len(x) >= 25)\n",
    "\n",
    "dog_breed['dog_breed'].value_counts().plot(kind = 'barh')\n",
    "plt.title('Most Rated Dog Breed')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Breed of dog');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Analyze and Visualize**: Visualizing the most popular dog names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.name.value_counts()[0:7].plot('barh', figsize=(15,8), title='Most Common Dog Names').set_xlabel(\"Number of Dogs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three most popular dog names are:\n",
    "1. Lucy - 11\n",
    "2. Charlie - 11\n",
    "3. Oliver - 10 and so on"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
